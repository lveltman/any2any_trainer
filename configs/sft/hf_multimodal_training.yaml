# Multimodal configuration with simplified approach to HuggingFace models
# Use standard models directly

# === BASIC PARAMETERS ===
model_name_or_path: "microsoft/DialoGPT-medium"  # Base language model
model_type: "multimodal"

# === MODALITIES ===
modalities:
  input: ["text", "image"]
  output: ["text"]

# === ENCODERS (standard HF models) ===
encoders:
  image:
    model: "openai/clip-vit-base-patch32"  # Standard CLIP model from HF
    freeze: true
    tokenizer_type: "continuous"

# === PROJECTION LAYER ===
projection:
  type: "mlp"
  hidden_size: 768  # DialoGPT hidden size
  num_layers: 2
  dropout: 0.1

# === SPECIAL TOKENS ===
special_tokens:
  image_start: "<image>"
  image_end: "</image>"

# === DATASET ===
dataset:
  - "liuhaotian/LLaVA-Instruct-150K"  # Standard dataset

conversation_field: "conversations"
image_field: "image"
max_seq_length: 2048

# === TRAINING PARAMETERS ===
output_dir: "./output/hf_multimodal_model"
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 8
num_train_epochs: 3
learning_rate: 2e-5
warmup_steps: 100
logging_steps: 10
save_steps: 500
eval_steps: 500
save_total_limit: 3

# === PEFT/LoRA (only for language model) ===
use_peft: true
lora:
  r: 64
  alpha: 128
  dropout: 0.1
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"
  bias: "none"

# === FREEZING ===
freeze_vision_encoder: true  # CLIP frozen
freeze_llm: false           # Language model trains (with LoRA)
train_projection_only: false

# === OPTIMIZATION ===
gradient_checkpointing: true
bf16: true
dataloader_num_workers: 2
remove_unused_columns: false

# === LOGGING ===
report_to: "none"
run_name: "hf_multimodal_training"

# === EXAMPLE GENERATION ===
generate_eval_examples: true
max_new_tokens: 256

# === OTHER ===
seed: 42 