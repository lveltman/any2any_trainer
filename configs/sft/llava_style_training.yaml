# Configuration for training LLaVA-style model (image + text â†’ text)

# Basic model parameters
model_name_or_path: "microsoft/DialoGPT-medium"
model_type: "multimodal"  # multimodal, any2any, unified

# Modality configuration
modalities:
  input: ["image", "text"]
  output: ["text"]

# Encoders for input modalities
encoders:
  image:
    model: "openai/clip-vit-base-patch32"
    freeze: true
    tokenizer_type: "continuous"
    processor_kwargs:
      do_rescale: true
      do_normalize: true

# Projection layers
projection:
  type: "mlp"  # mlp, linear, transformer
  hidden_size: 4096
  num_layers: 2
  dropout: 0.1

# Special tokens
special_tokens:
  image_start: "<image>"
  image_end: "</image>"

# Dataset configuration
dataset:
  - "liuhaotian/LLaVA-Instruct-150K"

conversation_field: "conversations"
image_field: "image"
max_seq_length: 2048

# Training parameters
output_dir: "./output/llava_style_model"
per_device_train_batch_size: 2
per_device_eval_batch_size: 2
gradient_accumulation_steps: 4
num_train_epochs: 3
learning_rate: 2e-5
warmup_steps: 100
logging_steps: 10
save_steps: 500
eval_steps: 500
save_total_limit: 3

# LoRA configuration
use_peft: true
lora:
  r: 64
  alpha: 128
  dropout: 0.1
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"
  bias: "none"

# Component freezing
freeze_vision_encoder: true
freeze_llm: false
train_projection_only: false

# Additional parameters
gradient_checkpointing: true
bf16: true
fp16: false
dataloader_num_workers: 4
remove_unused_columns: false

# Logging
report_to: "none"  # none, wandb, clearml, tensorboard
run_name: "llava_style_training"

# Generate examples during validation
generate_eval_examples: true
max_new_tokens: 256

# Seed for reproducibility
seed: 42 